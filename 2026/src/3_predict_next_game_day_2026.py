# -*- coding: utf-8 -*-
"""
NBA Game Prediction Script (2025-26 Season)

This script (Script 3 of 5) calculates predictions for the next NBA game day
based on historical data, rolling averages, and a LightGBM model. It reads
the latest game schedule generated by the ``2_get_data_next_game_day_2026``
script, merges it with season statistics and odds data, and writes a
prediction CSV into the ``PREDICTION_DIR`` defined in ``nba_utils_2026``.

Workflow:
1. Load most recent season stats (script 1 output) + upcoming games (script 2 output)
2. Build rolling features, normalize stats
3. Train LightGBM on historical matchups
4. Predict win probabilities for today's games
5. Pull betting odds and compute value
6. Save compact prediction CSV

Important notes:
- Team code normalization (CHA vs CHO, PHX vs PHO, BRK vs BKN) is applied
  consistently so WAS–CHA style games don't get dropped.
- We explicitly DROP 'won' from features to avoid label leakage.
"""

import os
import glob
import logging
from datetime import datetime
from typing import Tuple, List, Dict

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import lightgbm as lgb
import requests
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# Hard-coded API key for the-odds-api.com.
# (Replace this with env var if you want.)
API_KEY = "8e9d506f8573b01023028cef1bf645b5"

# Import shared utilities from the 2026 version
from nba_utils_2026 import (
    CURRENT_SEASON,
    ROLLING_WINDOW_SIZE,
    get_current_date,
    get_directory_paths,
    get_latest_file,
    preprocess_nba_data,
    calculate_rolling_averages,
    add_next_game_columns,
    impute_prob,
    am_to_dec,
    normalize_team_codes_inplace,
)

TEAM_ALIAS_FOR_ODDS = {
    # normalize everything to your internal codes
    "PHO": "PHX",
    "PHX": "PHX",
    "BKN": "BRK",
    "BRK": "BRK",
    "CHA": "CHO",   # sportsbooks use CHA, you use CHO
    "CHO": "CHO",
    "GS":  "GSW",
    "GSW": "GSW",
    "NO":  "NOP",
    "NOP": "NOP",
    "NY":  "NYK",
    "NYK": "NYK",
    "SA":  "SAS",
    "SAS": "SAS",
    "UTAH": "UTA",
    "UTA": "UTA",
    "OKL": "OKC",
    "OKC": "OKC",
    # most others (BOS, MIL, etc.) match already
}

def normalize_code_for_odds(abbr: str) -> str:
    """Take any team abbr from API or our data and return our internal code."""
    if not isinstance(abbr, str):
        return abbr
    return TEAM_ALIAS_FOR_ODDS.get(abbr.upper(), abbr.upper())


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


# -----------------------------------------------------------------------------
# Setup paths and input files
# -----------------------------------------------------------------------------
def setup_paths_and_files() -> Tuple[Dict[str, str], Tuple[datetime, str, str], str, str, pd.DataFrame]:
    """Locate needed input files and load them.

    Returns:
        (paths,
         (today_dt, today_friendly_str, today_ymd_str),
         df_path_stat,
         games_df_path,
         games_df)
    """
    # We'll consider "today" the prediction day
    today, today_str, today_str_format = get_current_date(0)

    paths = get_directory_paths()
    stat_dir = paths['STAT_DIR']
    next_game_dir = paths['NEXT_GAME_DIR']

    # --- season stats file (from script 1)
    df_path_stat = os.path.join(stat_dir, f"nba_games_{today_str_format}.csv")
    if not os.path.exists(df_path_stat):
        logging.info(
            f"Statistics file not found for today ({today_str_format}); "
            f"searching for latest file in {stat_dir}…"
        )
        stat_files = sorted(glob.glob(os.path.join(stat_dir, "nba_games_*.csv")))
        if stat_files:
            df_path_stat = stat_files[-1]
            logging.info(f"Using latest statistics file: {df_path_stat}")
        else:
            raise FileNotFoundError(f"No statistics files found in {stat_dir}")

    # --- upcoming games file (from script 2)
    games_df_path = get_latest_file(next_game_dir, prefix="games_df_", ext=".csv")
    if not games_df_path:
        raise FileNotFoundError(f"No games_df_*.csv found in {next_game_dir}")

    games_df = pd.read_csv(games_df_path)
    if 'Unnamed: 0' in games_df.columns:
        games_df = games_df.drop(columns=['Unnamed: 0'])

    logging.info(f"Loaded game schedule from {games_df_path} with {len(games_df)} games")

    return paths, (today, today_str, today_str_format), df_path_stat, games_df_path, games_df


# -----------------------------------------------------------------------------
# Data prep: stats → scaled + rolling features → add next game cols
# -----------------------------------------------------------------------------
def load_and_preprocess_data(
    df_path_stat: str,
    games_df: pd.DataFrame
) -> Tuple[pd.DataFrame, List[str], Dict[str, str]]:
    """
    Load historical NBA team-level game rows, add target,
    scale numeric stats, generate rolling window features,
    and append next game info.

    Returns:
        df (full enriched DataFrame),
        selected_columns (original normalized feature cols),
        rolling_cols (mapping original→rolling col names)
    """
    # Core cleaned dataset
    df = preprocess_nba_data(df_path_stat)

    # Which cols we do NOT scale
    removed_columns = ["season", "date", "won", "target", "team", "team_opp"]

    # We'll scale everything else
    selected_columns = df.columns[~df.columns.isin(removed_columns)]

    scaler = MinMaxScaler()
    df[selected_columns] = scaler.fit_transform(df[selected_columns])

    # Rolling means over last ROLLING_WINDOW_SIZE games per team/season
    df_rolling = calculate_rolling_averages(
        df[selected_columns.tolist() + ["won", "team", "season"]],
        ROLLING_WINDOW_SIZE
    )

    # Suffix rolling stats with _{window}
    rolling_cols = {
        col: f"{col}_{ROLLING_WINDOW_SIZE}"
        for col in df_rolling.columns
        if col not in ['team', 'season']
    }
    df_rolling = df_rolling.rename(columns=rolling_cols)

    # Join rolling stats back in
    df = pd.concat(
        [df.reset_index(drop=True), df_rolling.reset_index(drop=True)],
        axis=1
    ).dropna()

    # Add next-game info columns ('home_next', 'team_opp_next', etc.)
    df = add_next_game_columns(df)

    return df, selected_columns.tolist(), rolling_cols


# -----------------------------------------------------------------------------
# Build train and prediction matrices
# -----------------------------------------------------------------------------
def prepare_training_and_prediction_data(
    df: pd.DataFrame,
    games_df: pd.DataFrame,
    game_day: str
) -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:
    """
    Build:
      - train_df: historical matchups (features_x vs features_y, + target)
      - pred_df: upcoming matchups for game_day (same columns but target=2)

    NOTE:
    We intentionally DROP 'won' from the feature sets to avoid label leakage.
    """
    logging.info("Preparing training and prediction datasets…")

    # We want the *latest snapshot per team* (most recent row in df per team)
    records = df.to_dict('records')
    team_latest: Dict[str, Dict] = {}
    for rec in records:
        team = str(rec.get('team'))
        date_val = rec.get('date')
        if not team:
            continue
        if isinstance(date_val, str):
            try:
                date_val = pd.to_datetime(date_val)
            except Exception:
                continue
        # keep the most recent row per team
        if team not in team_latest or date_val > team_latest[team]['date']:
            team_latest[team] = {'record': rec, 'date': date_val}

    # ---------------------------
    # Training set
    # ---------------------------
    historical_rows = []
    skip_cols = ['team', 'team_opp', 'target', 'home', 'won']

    for rec in records:
        # skip rows where target == 2 (future game placeholder)
        if rec.get('target') == 2:
            continue

        team = rec.get('team')
        opp = rec.get('team_opp')
        if not team or not opp:
            continue

        opp_entry = team_latest.get(str(opp))
        if not opp_entry:
            continue
        opp_rec = opp_entry['record']

        row = {
            'team_x': str(team),
            'team_y': str(opp),
            'target': rec.get('target', 0),
            'home': rec.get('home', 0),
        }

        # add team features with _x suffix
        for col, val in rec.items():
            if col in skip_cols:
                continue
            row[f"{col}_x"] = val

        # add opponent features with _y suffix
        for col, val in opp_rec.items():
            if col in skip_cols:
                continue
            row[f"{col}_y"] = val

        historical_rows.append(row)

    train_df = pd.DataFrame(historical_rows)
    logging.info(f"Training dataset size: {len(train_df)} rows")

    # ---------------------------
    # Prediction set for upcoming games
    # ---------------------------
    prediction_rows = []
    for _, game in games_df.iterrows():
        home_team = str(game['home_team'])
        away_team = str(game['away_team'])

        home_entry = team_latest.get(home_team)
        away_entry = team_latest.get(away_team)
        if not home_entry or not away_entry:
            logging.warning(f"Missing data for {home_team} vs {away_team}; skipping")
            continue

        home_rec = home_entry['record']
        away_rec = away_entry['record']

        pred_row = {
            'team_x': home_team,
            'team_y': away_team,
            'target': 2,          # mark as future / not known
            'home': 1,            # always 1 because we're modeling the home team win prob
            'date_next': game_day,
        }

        # add home team features with _x suffix
        for col, val in home_rec.items():
            if col in skip_cols:
                continue
            pred_row[f"{col}_x"] = val

        # add away team features with _y suffix
        for col, val in away_rec.items():
            if col in skip_cols:
                continue
            pred_row[f"{col}_y"] = val

        prediction_rows.append(pred_row)

    pred_df = pd.DataFrame(prediction_rows)
    logging.info(f"Prediction dataset size: {len(pred_df)} rows")

    # Columns we should never include in model features
    removed_columns = [
        'team_x', 'team_y', 'target', 'home', 'date', 'date_next',
        'season', 'won', 'team', 'team_opp'
    ]

    return train_df, pred_df, removed_columns


# -----------------------------------------------------------------------------
# Train LightGBM model
# -----------------------------------------------------------------------------
def train_model(train_df: pd.DataFrame, removed_columns: List[str]) -> Tuple[lgb.LGBMClassifier, List[str]]:
    """
    Train a LightGBM model on historical rows.
    """
    logging.info(
        f"Training data contains {len(train_df)} rows "
        f"and {len(train_df.columns)} columns"
    )
    if len(train_df) < 10:
        raise ValueError("Insufficient training data; need at least 10 historical games")

    # Figure out which columns are allowed as numeric features
    to_drop = set(removed_columns)

    for col in train_df.columns:
        if col in to_drop:
            continue

        # Drop non-numeric columns
        if train_df[col].dtype == object:
            # attempt parse datetime; if it parses, it's also not a numeric feature
            try:
                pd.to_datetime(train_df[col].iloc[0])
                to_drop.add(col)
                continue
            except Exception:
                to_drop.add(col)
                continue

        # Drop any weird non-numeric dtypes
        if not pd.api.types.is_numeric_dtype(train_df[col]):
            to_drop.add(col)

    feature_cols = [c for c in train_df.columns if c not in to_drop]

    if not feature_cols:
        raise ValueError("No valid numeric features available for training")

    X = train_df[feature_cols].values
    y = train_df['target'].values

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42
    )

    params = {
        'objective': 'binary',
        'metric': 'auc',
        'num_leaves': 10,
        'learning_rate': 0.1,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.9,
        'bagging_freq': 10,
        'boosting_type': 'gbdt',
        'verbosity': -1,
        'random_state': 42,
        'lambda_l1': 0.5,
        'lambda_l2': 0.5,
        'max_depth': 7,
        'min_child_weight': 5,
    }

    model = lgb.LGBMClassifier(**params)
    model.fit(X_train, y_train)

    pred_test = model.predict(X_test)
    accuracy = accuracy_score(y_test, pred_test)
    logging.info(f"LightGBM model trained with accuracy: {accuracy:.2%}")

    # Feature importance logging (top 10)
    importances = model.feature_importances_
    feat_imp = sorted(
        zip(feature_cols, importances),
        key=lambda x: x[1],
        reverse=True
    )
    top_features = feat_imp[: min(10, len(feat_imp))]
    logging.info("Top feature importances (first 10):")
    for i, (name, score) in enumerate(top_features, 1):
        logging.info(f"  {i}. {name}: {score}")

    return model, feature_cols


# -----------------------------------------------------------------------------
# Predict for upcoming games
# -----------------------------------------------------------------------------
def predict_upcoming_games(
    model: lgb.LGBMClassifier,
    pred_df: pd.DataFrame,
    feature_cols: List[str],
    game_day: str
) -> pd.DataFrame:
    """
    Use trained model to predict P(home team wins) for each upcoming game.
    """
    if pred_df.empty:
        logging.warning("No games to predict")
        return pd.DataFrame()

    X_pred = pred_df[feature_cols].values
    proba = model.predict_proba(X_pred)[:, 1]

    # Build result frame
    results = []
    for i, (_, row) in enumerate(pred_df.iterrows()):
        results.append({
            'home_team': row['team_x'],
            'away_team': row['team_y'],
            'home_team_prob': float(proba[i]),
            'result': 0,          # not known yet
            'date': game_day,
        })
    return pd.DataFrame(results)


# -----------------------------------------------------------------------------
# Odds fetching
# -----------------------------------------------------------------------------
def get_session() -> requests.Session:
    """Create a requests session with retry configuration."""
    session = requests.Session()
    retries = Retry(
        total=3,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504]
    )
    session.mount("https://", HTTPAdapter(max_retries=retries))
    return session


def fetch_odds(games_df: pd.DataFrame, api_key: str, preferred: List[str] = None) -> pd.DataFrame:
    """
    Fetch head-to-head odds from the-odds-api.com and align team codes
    with our internal abbreviations (e.g. CHA -> CHO, BKN -> BRK).
    """

    # Map full team names (what the API returns) to abbreviations
    full_to_abbrev = {
        "Atlanta Hawks": "ATL",
        "Boston Celtics": "BOS",
        "Brooklyn Nets": "BRK",          # we use BRK
        "Charlotte Hornets": "CHO",      # <-- important: force CHO here
        "Chicago Bulls": "CHI",
        "Cleveland Cavaliers": "CLE",
        "Dallas Mavericks": "DAL",
        "Denver Nuggets": "DEN",
        "Detroit Pistons": "DET",
        "Golden State Warriors": "GSW",
        "Houston Rockets": "HOU",
        "Indiana Pacers": "IND",
        "Los Angeles Clippers": "LAC",
        "LA Clippers": "LAC",
        "Los Angeles Lakers": "LAL",
        "Memphis Grizzlies": "MEM",
        "Miami Heat": "MIA",
        "Milwaukee Bucks": "MIL",
        "Minnesota Timberwolves": "MIN",
        "New Orleans Pelicans": "NOP",
        "New York Knicks": "NYK",
        "Oklahoma City Thunder": "OKC",
        "Orlando Magic": "ORL",
        "Philadelphia 76ers": "PHI",
        "Phoenix Suns": "PHX",
        "Portland Trail Blazers": "POR",
        "Sacramento Kings": "SAC",
        "San Antonio Spurs": "SAS",
        "Toronto Raptors": "TOR",
        "Utah Jazz": "UTA",
        "Washington Wizards": "WAS",
    }

    session = get_session()
    response = session.get(
        "https://api.the-odds-api.com/v4/sports/basketball_nba/odds",
        params={
            "apiKey": api_key,
            "regions": "us",
            "markets": "h2h",
            "oddsFormat": "american",
        },
        timeout=10,
    )
    response.raise_for_status()
    data = response.json()

    # Build lookup where keys are *our normalized codes*
    # lookup[(home_code, away_code)] = (price_home, price_away)
    lookup = {}

    for event in data:
        home_full = event.get("home_team")
        away_full = event.get("away_team")

        # convert "Charlotte Hornets" -> "CHO", etc.
        raw_home_abbr = full_to_abbrev.get(home_full)
        raw_away_abbr = full_to_abbrev.get(away_full)
        if not raw_home_abbr or not raw_away_abbr:
            continue

        # normalize aliases like CHA->CHO, BKN->BRK, etc.
        home_abbr = normalize_code_for_odds(raw_home_abbr)
        away_abbr = normalize_code_for_odds(raw_away_abbr)

        bookmakers = event.get("bookmakers", [])
        chosen = None
        if preferred:
            for p in preferred:
                chosen = next((b for b in bookmakers if b.get("key") == p), None)
                if chosen:
                    break
        if not chosen and bookmakers:
            chosen = bookmakers[0]

        if not chosen:
            continue

        market = next(
            (m for m in chosen.get("markets", []) if m.get("key") == "h2h"),
            None
        )
        if not market:
            continue

        # Grab the American moneylines for each side
        prices = {}
        for outcome in market.get("outcomes", []):
            name_full = outcome.get("name")  # e.g. "Charlotte Hornets"
            abbr_raw = full_to_abbrev.get(name_full)
            if abbr_raw:
                abbr_norm = normalize_code_for_odds(abbr_raw)
                prices[abbr_norm] = outcome.get("price")

        price_home = prices.get(home_abbr)
        price_away = prices.get(away_abbr)

        lookup[(home_abbr, away_abbr)] = (price_home, price_away)

    # Now build the odds_rows aligned to the games_df
    odds_rows = []
    for _, gm in games_df.iterrows():
        # normalize our own team codes the same way
        h = normalize_code_for_odds(gm['home_team'])
        a = normalize_code_for_odds(gm['away_team'])

        o1, o2 = lookup.get((h, a), (None, None))
        if o1 is None or o2 is None:
            logging.warning(f"No odds found for {h} vs {a}")

        odds_rows.append({
            "home_team": h,
            "away_team": a,
            "odds 1": o1,
            "odds 2": o2
        })

    return pd.DataFrame(odds_rows)


    session = get_session()
    response = session.get(
        "https://api.the-odds-api.com/v4/sports/basketball_nba/odds",
        params={
            "apiKey": api_key,
            "regions": "us",
            "markets": "h2h",
            "oddsFormat": "american",
        },
        timeout=10,
    )
    response.raise_for_status()
    data = response.json()

    # Build lookup dict with normalized abbrevs
    lookup = {}
    for event in data:
        home_full = event.get("home_team")
        away_full = event.get("away_team")
        home_abbr = full_to_abbrev.get(home_full)
        away_abbr = full_to_abbrev.get(away_full)
        if not home_abbr or not away_abbr:
            continue

        bookmakers = event.get("bookmakers", [])
        chosen = None
        if preferred:
            for p in preferred:
                chosen = next(
                    (b for b in bookmakers if b.get("key") == p),
                    None
                )
                if chosen:
                    break
        if not chosen and bookmakers:
            chosen = bookmakers[0]

        if not chosen:
            continue

        market = next(
            (m for m in chosen.get("markets", []) if m.get("key") == "h2h"),
            None
        )
        if not market:
            continue

        prices = {}
        for outcome in market.get("outcomes", []):
            name = outcome.get("name")
            abbr = full_to_abbrev.get(name)
            if abbr:
                prices[abbr] = outcome.get("price")

        lookup[(home_abbr, away_abbr)] = (
            prices.get(home_abbr),
            prices.get(away_abbr),
        )

    # Attach odds to each game in games_df
    odds_rows = []
    for _, gm in games_df.iterrows():
        h, a = gm['home_team'], gm['away_team']
        o1, o2 = lookup.get((h, a), (None, None))
        if o1 is None or o2 is None:
            logging.warning(f"No odds found for {h} vs {a}")
        odds_rows.append(
            {"home_team": h, "away_team": a, "odds 1": o1, "odds 2": o2}
        )

    return pd.DataFrame(odds_rows)


# -----------------------------------------------------------------------------
# Merge predictions with odds and compute value
# -----------------------------------------------------------------------------
def merge_predictions_with_odds(preds: pd.DataFrame, odds: pd.DataFrame) -> pd.DataFrame:
    """
    Combine model predictions with odds, normalize team codes BEFORE merge,
    and compute implied probabilities & value.
    """
    preds = preds.copy()
    odds = odds.copy()

    # normalize any lingering legacy codes (safety net)
    for frame in (preds, odds):
        frame["home_team"] = frame["home_team"].replace({"PHO": "PHX", "CHO": "CHA"})
        frame["away_team"] = frame["away_team"].replace({"PHO": "PHX", "CHO": "CHA"})

    df = preds.merge(odds, on=["home_team", "away_team"], how="left")

    # convert odds cols to numeric
    for col in ["odds 1", "odds 2"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    # implied probabilities (moneyline -> prob)
    df["imp_prob_home"] = df["odds 1"].apply(impute_prob)
    df["imp_prob_away"] = df["odds 2"].apply(impute_prob)

    # value = model edge vs market
    df["value_home"] = np.where(
        df["imp_prob_home"].notna(),
        df["home_team_prob"] - df["imp_prob_home"],
        np.nan
    )
    df["value_away"] = np.where(
        df["imp_prob_away"].notna(),
        (1.0 - df["home_team_prob"]) - df["imp_prob_away"],
        np.nan
    )

    return df


def prepare_final_output(
    predictions: pd.DataFrame,
    odds_df: pd.DataFrame,
    game_day: str
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    final_df:
        predictions + odds + value calcs
    home_team_preds:
        compact table, with decimal odds
    """
    final_df = merge_predictions_with_odds(predictions, odds_df)

    # turn odds into decimal format for the export sheet
    home_team_preds = predictions.merge(
        odds_df[["home_team", "away_team", "odds 1", "odds 2"]],
        on=["home_team", "away_team"],
        how="left"
    )

    home_team_preds["odds 1"] = home_team_preds["odds 1"].apply(am_to_dec)
    home_team_preds["odds 2"] = home_team_preds["odds 2"].apply(am_to_dec)

    home_team_preds["odds 1"] = home_team_preds["odds 1"].apply(
        lambda x: round(x, 2) if pd.notnull(x) else x
    )
    home_team_preds["odds 2"] = home_team_preds["odds 2"].apply(
        lambda x: round(x, 2) if pd.notnull(x) else x
    )

    home_team_preds = home_team_preds.assign(
        result=0,
        date=game_day
    )

    return final_df, home_team_preds


# -----------------------------------------------------------------------------
# Save output
# -----------------------------------------------------------------------------
def save_predictions(
    home_team_preds: pd.DataFrame,
    directory: str,
    today_str_format: str
) -> str:
    """
    Write the compact prediction CSV that the betting workflow consumes.
    """
    filename = f"nba_games_predict_{today_str_format}.csv"
    filepath = os.path.join(directory, filename)
    os.makedirs(directory, exist_ok=True)

    cols = [
        "home_team",
        "away_team",
        "home_team_prob",
        "result",
        "odds 1",
        "odds 2",
        "date",
    ]

    if os.path.exists(filepath):
        logging.info(f"Prediction file already exists: {filepath}")
    else:
        home_team_preds[cols].to_csv(filepath, index=False)
        logging.info(f"Saved predictions to {filepath}")

    return filepath


# -----------------------------------------------------------------------------
# Main pipeline
# -----------------------------------------------------------------------------
def main() -> str:
    """Run full prediction pipeline and return saved CSV path."""
    # 1. Paths + load data
    paths, (today, today_str, today_str_format), df_path_stat, games_df_path, games_df = setup_paths_and_files()

    # 2. Load & preprocess historical stats
    df, selected_columns, rolling_cols = load_and_preprocess_data(df_path_stat, games_df)

    # 3. Normalize team codes in BOTH df and games_df
    #    This resolves CHA/CHO etc. before we build train/pred sets.
    normalize_team_codes_inplace(df, ["team", "team_opp"])
    normalize_team_codes_inplace(games_df, ["home_team", "away_team"])

    # 4. Build train/predict matrices
    train_df, pred_df, removed_cols = prepare_training_and_prediction_data(df, games_df, today_str_format)

    # 5. Train model
    model, feature_cols = train_model(train_df, removed_cols)

    # 6. Predict probabilities for today's games
    home_team_preds_ml = predict_upcoming_games(model, pred_df, feature_cols, today_str_format)

    # 7. Fetch odds with consistent team mapping (CHA not CHO)
    odds_df = fetch_odds(games_df, API_KEY, preferred=["draftkings", "fanduel"])

    # 8. Merge predictions with odds and compute value
    final_df, home_team_preds = prepare_final_output(home_team_preds_ml, odds_df, today_str_format)

    # 9. Save predictions file
    output_path = save_predictions(home_team_preds, paths['PREDICTION_DIR'], today_str_format)

    logging.info(f"Prediction CSV saved to {output_path}")
    return output_path


if __name__ == "__main__":
    try:
        main()
    except Exception:
        logging.exception("An unexpected error occurred during prediction.")
    finally:
        # Keep console window open for local Windows usage.
        # In headless mode (e.g. GitHub Actions) input() will raise EOFError.
        try:
            input("Prediction complete. Press Enter to close this window...")
        except EOFError:
            pass
